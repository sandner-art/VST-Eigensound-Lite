Eigensound: A Computationally Efficient Approach to Physical Modeling Synthesis and a Proposed Protocol for its Multi-Faceted EvaluationAbstractThis paper introduces Eigensound, a novel Digital Musical Instrument (DMI) designed to address the persistent trade-off between expressive potential and computational efficiency in real-time sound synthesis. We present the formal architecture of the Eigensound system, detailing its unique physical modeling engine which leverages a computationally efficient, real-time modal resonator coupling model. The primary contributions of this work are twofold: first, the formal presentation of the Eigensound instrument as a novel approach to interactive sound synthesis; and second, a comprehensive, three-part experimental protocol for its evaluation. This proposed methodology combines perceptual evaluation of synthesis quality, mixed-methods assessment of usability, and a quantitative investigation of expressive potential, providing a rigorous framework for validating the instrument's claims and guiding its future development. This work contributes both a new DMI and a robust evaluation plan that can serve as a model for similar research.1.0 IntroductionThe design of new digital musical instruments (DMIs) is a field driven by a central, persistent challenge: the creation of instruments that are simultaneously rich in expressive potential and computationally tractable for real-time performance.1 Historically, sound synthesis has navigated a spectrum of compromises. Sampling technology, while capable of high-fidelity reproduction, often results in static, unresponsive instruments that struggle to capture the continuous nuances of acoustic performance.2 Conversely, abstract synthesis methods like Frequency Modulation (FM) offer vast sonic palettes but often with control paradigms that are unintuitive and disconnected from the physical gestures musicians cultivate.3Physical Modeling (PM) synthesis emerged as a powerful paradigm to address this gap. By simulating the underlying physics of sound-producing objects, PM offers the potential for highly dynamic, responsive, and realistic sounds that react organically to performer input.2 The foundational promise of PM is to move beyond mere sound reproduction towards the simulation of behavior, allowing for a level of interactivity that mirrors acoustic instruments.4 However, the history of PM is also a history of managing computational complexity. Early, highly accurate simulation methods based on finite difference approximations of the wave equation were computationally prohibitive for real-time use.4 This computational burden spurred the development of more efficient algorithms, such as the seminal Karplus-Strong plucked-string model and Julius O. Smith’s highly influential Digital Waveguide synthesis, which made real-time PM a practical reality.3The Eigensound instrument, presented in this paper, is a direct response to this ongoing challenge. It is built upon a novel physical modeling engine that aims to deliver a high degree of physical verisimilitude and expressive control while maintaining remarkable computational efficiency, making it suitable for a wide range of performance contexts.This paper offers two primary contributions to the field. First, it provides a formal architectural and algorithmic description of the Eigensound DMI, detailing its specific innovations in physical modeling synthesis and interaction design. Second, and equally significant, it proposes a comprehensive and scientifically-grounded experimental design for the instrument's evaluation. This multi-faceted protocol is presented not merely as a future plan, but as a robust methodological framework in its own right, designed to rigorously assess the instrument's sonic quality, usability, and expressive capacity.2.0 Background and Related WorkTo situate the contributions of Eigensound, it is essential to first survey the current landscape of physical modeling synthesis, analyze contemporary instruments, and understand the established principles of DMI evaluation.2.1 The Landscape of Physical Modeling SynthesisThe field of physical modeling has evolved from a niche academic pursuit into a diverse and commercially viable set of techniques for sound synthesis. This evolution has been marked by the development of increasingly sophisticated and efficient algorithms.2.1.1 Foundational PrinciplesThe practical application of physical modeling in real-time systems was catalyzed by two key developments. The Karplus-Strong algorithm, first described in 1983, provided a remarkably simple yet effective method for simulating plucked strings using a filtered delay line fed with a noise burst.4 This technique laid the groundwork for a class of models based on feedback and delay.3 Building upon this, Digital Waveguide synthesis, pioneered by Julius O. Smith III, generalized the concept to model the propagation of waves in a one-dimensional medium, such as a string or an air column.4 By using delay lines to simulate wave travel and filters to model energy loss and reflection at boundaries, waveguide synthesis became a cornerstone of commercial PM synthesizers, most notably in Yamaha's VL series.4 These foundational methods established the core principle of simplifying complex physical systems into computationally manageable digital structures.2.1.2 Modern MethodologiesThe contemporary landscape of PM is characterized by a diversification of these foundational ideas, with several distinct approaches coexisting:Modal Synthesis: This technique is based on the principle that the sound of a vibrating object can be decomposed into a set of resonant modes, each with its own frequency, damping, and amplitude.10 The sound is synthesized by summing the outputs of a bank of parallel resonant filters, excited by a signal representing the initial energy input. This approach has proven to be highly effective and, for certain classes of sounds like impacts, perceptually indistinguishable from acoustic recordings.10Mass-Spring Interaction Networks: This method models a physical object as a network of point masses connected by springs, each with defined physical properties like weight and stiffness.8 The system's behavior is governed by simulating the forces of tension and compression according to the laws of motion. This approach can model highly complex, non-linear, and even chaotic behaviors, offering a wide range of sonic possibilities, as exemplified in instruments like Baby Audio's Atoms synthesizer.5Finite Difference Time-Domain (FDTD) / Finite Element Methods (FEM): These are among the most computationally intensive yet physically accurate simulation techniques. They involve discretizing the object and the surrounding space into a grid and solving the differential equations of motion or wave propagation at each point over discrete time steps.6 While often too demanding for real-time polyphonic performance on standard hardware, they are used in advanced research, such as the NESS (Next Generation Sound Synthesis) project, to achieve unparalleled simulation fidelity.122.1.3 The Realism-Malleability SpectrumThe diversification of PM techniques has led to a philosophical and practical divergence in instrument design, which can be understood as a spectrum between realistic emulation and creative malleability. This spectrum is not a simple trade-off but a reflection of different design goals and artistic affordances.On one end of this spectrum lie instruments designed for hyper-realistic emulation of existing acoustic instruments. Products like Audio Modeling's SWAM engine and Modartt's Pianoteq are lauded for their ability to reproduce the nuanced behaviors of solo strings, winds, and pianos with extraordinary fidelity.13 The primary goal of these instruments is to provide performers with a digital surrogate that responds with the same expressive depth and detail as its acoustic counterpart. Their innovation lies in the sophistication of their "behavior modeling," which captures subtle performance articulations like bow changes, breath pressure, and sympathetic resonance, often requiring specialized controllers like MPE (MIDI Polyphonic Expression) or breath controllers to be fully realized.13On the other end of the spectrum are instruments that leverage the principles of physical modeling not for pure emulation, but as a springboard for creating novel and "otherworldly" sounds.17Applied Acoustics Systems' Chromaphone and Baby Audio's Atoms are prime examples of this approach.11 Chromaphone's core concept involves coupling disparate resonator types—such as a string with a metal plate or a tube with a drumhead—to create physically impossible yet sonically compelling hybrid instruments.18 Similarly, Atoms allows users to manipulate the parameters of its mass-spring network to create organic, evolving textures that sound both natural and artificial simultaneously.11 For these instruments, the value is not in replicating the known, but in exploring the sonically unknown through the intuitive and coherent framework that physical laws provide.Positioning a new DMI like Eigensound within this spectrum is a critical step in defining and defending its novelty. Its contribution may lie in achieving a higher degree of realism with greater computational efficiency than existing emulators, or it may lie in offering a unique set of creative, non-emulative sonic possibilities derived from its specific architectural innovations. This positioning directly informs the instrument's technical description and dictates the most appropriate strategies for its evaluation.2.2 A Comparative Analysis of Contemporary PM InstrumentsTo make the novelty of Eigensound concrete and defensible, it is essential to compare it directly with the leading commercial and academic systems that define the state-of-the-art. The following table provides a concise analysis, positioning Eigensound relative to its most significant peers across several key dimensions. This systematic comparison highlights the specific design space that Eigensound aims to occupy.Table 1: Comparative Analysis of Physical Modeling Synthesizers| Instrument/System | Core Synthesis Method | Primary Interaction Paradigm | Expressive Domain | Stated Novelty / Key Feature || :--- | :--- | :--- | :--- | :--- || Eigensound Lite (This Work) | Real-time Modal Resonator Coupling | Multi-touch surface, gestural control | Percussive, sustained resonant textures | Computationally efficient simulation of non-linear interactions || AAS Chromaphone 3 | Coupled Resonator Synthesis (Modal) 20 | Keyboard / MIDI | Percussive, Mallets, Creative Textures | Coupling of disparate resonator types (e.g., string and plate) 18 || Modartt Pianoteq 8 | Advanced Physical Modeling 22 | Keyboard / MIDI | Pianos, Mallets, Harpsichords | High-realism emulation with deep parameter control; extremely small footprint 14 || Audio Modeling SWAM | Digital Waveguides & Behavioral Modeling 13 | MPE / Breath Controllers | Solo Strings, Winds, Brass | Unsurpassed real-time expressive control and seamless articulation transitions 15 || NESS Project | Time-stepping Finite Difference Methods 6 | Offline / Non-real-time | High-fidelity simulation of complex systems | Large-scale, non-simplified simulation for ultimate realism, not real-time performance |This table clarifies that while Eigensound shares a foundation in modal synthesis with an instrument like Chromaphone, its primary novelty claim lies in the computational efficiency of its specific model and its tight integration with a gestural, multi-touch interface, distinguishing it from the keyboard-centric paradigm of most commercial offerings.2.3 Evaluation of Digital Musical InstrumentsThe evaluation of DMIs has matured significantly, evolving in parallel with the broader field of Human-Computer Interaction (HCI). Understanding this evolution is crucial for designing a rigorous and relevant evaluation protocol for Eigensound.2.3.1 From Usability to User Experience (UX)Early DMI evaluations, heavily influenced by HCI, often focused on task-based usability metrics such as efficiency and error rate.1 While valuable, this approach proved insufficient for capturing the essence of musical interaction, which is inherently subjective and experiential. A performer's relationship with an instrument is not solely defined by its ease of use, but by its capacity for expression, its potential for virtuosity, and the quality of the aesthetic experience it affords.1 This recognition led to a shift within the NIME community towards methods inspired by User Experience (UX) design, which prioritize the user's subjective feelings, motivations, and engagement.1 This broader perspective acknowledges that factors like enjoyment, frustration, control intimacy, and creative flow are central to a DMI's success.12.3.2 Established Frameworks and ModelsTo structure these more complex evaluations, researchers have developed conceptual frameworks that provide a "scaffold" for organizing different approaches.26 A widely accepted model describes a DMI as comprising three essential, interconnected parts: a controller (the physical interface), a sound engine (the sound generation algorithm), and a mapping (the set of rules translating controller actions into sound engine parameters).30 This model is critical because it formalizes the understanding that the instrument is more than just its sound. The evaluation must consider the entire system and the interplay between its components. Furthermore, robust frameworks emphasize the need to evaluate from multiple stakeholder perspectives, including those of the designer, the performer, and the audience, as each group has different criteria for success.26The implications of this holistic view are profound. The most successful and influential DMIs are not merely sound generators; they are integrated systems where the interaction design is as crucial as the synthesis algorithm. The expressive power of the SWAM engine, for instance, is unlocked only through the use of highly expressive controllers that provide continuous, multi-dimensional input.13 This has led to the widely cited maxim that "the mapping is the essence of the instrument".27 Consequently, any credible evaluation of Eigensound cannot treat its synthesis engine in isolation. The proposed experimental protocol must be designed to assess the complete instrument: the quality of the sound it produces, the usability of its interactive controls, and the expressive potential that emerges from the fusion of the two.3.0 The Eigensound Instrument: Architecture and FormalismThis section provides the core technical contribution of the paper, presenting a detailed and formal description of the Eigensound DMI. The architecture is broken down into its constituent components, with a particular focus on the novel synthesis engine that forms the heart of the instrument.3.1 System ArchitectureThe Eigensound instrument adheres to the established three-part model of a DMI, comprising a controller, a sound engine, and a mapping layer that connects them.30 The integration of these components is designed to create a cohesive and responsive performance system.Controller: The primary controller for Eigensound Lite is a multi-touch surface, such as that found on a tablet or a dedicated hardware device. This interface was chosen to afford continuous, two-dimensional control over multiple parameters simultaneously, moving beyond the discrete, one-dimensional nature of a traditional keyboard.Sound Engine: The core of the instrument is the Eigensound synthesis engine, a novel implementation of physical modeling based on the real-time coupling of modal resonators. This engine is designed for high computational efficiency, allowing for complex, polyphonic textures to be generated on standard consumer hardware. A detailed formalism of the engine is provided in Section 3.2.Mapping: The mapping layer serves as the crucial bridge between the performer's actions and the sonic output. It translates gestural data from the multi-touch controller—such as touch position (x,y), pressure (p), and velocity of initial contact (v)—into control signals for the synthesis engine. This mapping is not arbitrary; it is designed to be intuitive and to foster a sense of direct manipulation of a virtual resonant object, a concept central to the instrument's design philosophy.27The overall signal and control flow of the Eigensound system is illustrated in the block diagram below.(A block diagram would be inserted here, showing a box for the "Multi-Touch Controller" with outputs for x, y, p, v. These outputs would feed into a central "Mapping Logic" box. The Mapping Logic box would then have outputs feeding into the "Eigensound Synthesis Engine," which in turn produces the final audio output.)3.2 The Eigensound Synthesis Engine: A FormalismThe technical novelty of Eigensound resides in its synthesis engine. This section provides a formal mathematical description of its underlying model and highlights the specific innovations that enable its performance.3.2.1 Underlying Physical ModelThe Eigensound engine is a form of modal synthesis.10 It models a vibrating object not by simulating its entire geometry, but by representing its sound as the sum of its fundamental modes of vibration. Each mode is a damped sinusoid with a specific frequency, decay rate, and amplitude. The sound is generated by exciting a bank of parallel digital resonators, typically implemented as second-order IIR (Infinite Impulse Response) filters, which correspond to these modes.The core equation for a single mode i can be expressed as:yi​(n)=ai​x(n)−b1,i​yi​(n−1)−b2,i​yi​(n−2)where yi​(n) is the output of the i-th resonator at time step n, x(n) is the excitation signal, and the filter coefficients b1,i​ and b2,i​ determine the mode's frequency and damping. The final output signal Y(n) is the sum of all modal outputs:Y(n)=i=1∑N​yi​(n)+R(n)where N is the total number of modes and R(n) is a residual signal, often stochastic (noise-based), that represents the non-harmonic components of the sound.103.2.2 Computational Novelty: Real-Time Resonator CouplingThe primary innovation of the Eigensound engine lies in its method for simulating the non-linear coupling between resonators in a computationally efficient manner. In many real-world objects, the vibration of one mode can transfer energy to and influence other modes. Accurately simulating this behavior, especially using complex FDTD or mass-spring models, is computationally expensive.6Eigensound implements a novel simplification. Instead of a full physical simulation of energy transfer, it employs a perceptually-motivated modulation matrix, C, where each element cij​ defines the influence of mode i on the amplitude of mode j. The amplitude gain gj​ for each mode j is updated at each time step based on the energy of all other modes:gj​(n)=gj,base​+i=1∑N​cij​Ei​(n−1)where gj,base​ is the base gain of the mode and Ei​(n−1) is a measure of the energy (e.g., the squared amplitude) of mode i at the previous time step.This approach avoids solving complex differential equations, replacing them with a series of multiplications and additions that are highly parallelizable and efficient on modern CPUs. This allows for the dynamic, evolving timbres characteristic of complex resonant interactions, but at a fraction of the computational cost of traditional high-fidelity methods. This efficiency is the central, defensible claim of the engine's novelty.3.3 Interaction and Mapping ParadigmThe mapping strategy in Eigensound is designed to create an intuitive and expressive link between the performer's gestural input and the parameters of the synthesis engine. The goal is to move beyond a simple triggering of sounds and towards a continuous, nuanced control over the sound's evolution.Mapping Strategy: The multi-touch input parameters are mapped to the synthesis engine as follows:Initial Touch (Tap): The velocity and pressure of the initial touch are mapped to the properties of the excitation signal (x(n)). A faster, harder tap creates a sharper, brighter excitation signal (e.g., a shorter, higher-amplitude noise burst), injecting more energy into the higher-frequency modes.Touch Position (x,y): The position of the touch on the surface is mapped to the base gain distribution across the modal bank (gj,base​). This simulates striking or bowing a virtual object at different locations, which excites different sets of modes. For example, touching near the center might excite lower-frequency modes, while touching near the edge excites higher-frequency ones.Sustained Pressure (p): Continuous pressure applied after the initial touch is mapped to the overall damping of the system (by modifying the b2​ coefficients of the filters). Increasing pressure can decrease damping, allowing the sound to sustain longer or even "swell" in a non-acoustic manner, providing a key dimension of expressive control.27Gestural Movement (Drag): The movement of the touch point across the surface dynamically updates the base gain distribution, simulating the effect of scraping or moving the excitation point across the virtual object's surface.Expressive Control: This mapping scheme is designed to directly afford control over fundamental dimensions of musical expression.31 Dynamics are controlled by the initial tap velocity and sustained pressure. Timbre is controlled by the touch position and the resulting modal excitation. Articulation, from sharp staccato taps to long, evolving drones, is a natural outcome of the duration and modulation of the touch itself. This tight coupling between action and sound is intended to make the instrument feel immediate, responsive, and highly expressive.4.0 Proposed Experimental Design for the Evaluation of EigensoundA core contribution of this paper is the proposal of a comprehensive and rigorous experimental protocol for the evaluation of the Eigensound DMI. Acknowledging that the success of a DMI is a multi-faceted construct, this protocol is divided into three distinct but complementary studies. Each study is designed to answer a specific research question, targeting a different aspect of the instrument's performance: the objective quality of its sonic output, the subjective experience of a novice user, and the expressive capacity it affords an expert performer. This approach moves beyond a single metric of success to build a holistic and nuanced understanding of the instrument's strengths and weaknesses, providing a robust foundation for its validation and future development.The following table provides a summary of the three-part evaluation protocol, outlining the core components of each proposed study. This structure serves as a roadmap for the detailed methodologies presented in the subsequent sections and highlights the comprehensive nature of the evaluation.Table 2: Summary of the Proposed Three-Part Evaluation Protocol| Study | Research Question | Participants | Core Methodology | Key Metrics / Outcomes || :--- | :--- | :--- | :--- | :--- || 1. Perceptual | Is the synthesis quality of Eigensound perceptually distinguishable from acoustic recordings of similar sounds? | General Listeners | ABX / A-B Discrimination Task 10 | Discrimination factor (d'), F-measure. || 2. Usability | How do novice users experience Eigensound in terms of usability, learnability, and emergent interaction? | Novice Users | Mixed-Methods: Qualitative Exploration 28 + System Usability Scale (SUS) 32 | Qualitative themes (Discourse Analysis), quantitative SUS score (0-100). || 3. Expressivity | To what extent does Eigensound afford expert performers control over key dimensions of musical expression? | Expert Musicians | Task-Based Performance Evaluation 31 | Likert-scale ratings on control of dynamics, articulation, timbre, etc. |4.1 Study 1: Perceptual Evaluation of Synthesis QualityThe first study addresses the most fundamental claim of any physical modeling synthesizer: the quality of its sound. However, "realism" is a subjective and often indefensible claim. A more rigorous, scientific approach is to test for perceptual indistinguishability. This study is therefore designed not to prove that Eigensound sounds "realistic," but to test the falsifiable null hypothesis that listeners cannot reliably distinguish its synthesized output from acoustic recordings of similar sound events. This reframes the evaluation from a subjective debate into an empirical investigation, following successful methodologies in the field.104.1.1 Proposed MethodologyDesign: The study will employ a forced-choice A-B discrimination task. This design is chosen for its simplicity and statistical power in assessing the distinguishability of two stimuli.Stimuli: A set of audio stimuli will be prepared. For each target sound (e.g., a struck metal bar, a plucked string, a tapped wooden block), two versions will be created:An acoustic recording of the real-world event, captured in a controlled anechoic environment.A synthesized version created using the Eigensound engine, with its parameters tuned to emulate the acoustic recording as closely as possible.All audio files will be normalized to a consistent loudness level (e.g., -23 LUFS) to prevent loudness bias. Additionally, following the methodology of Farnell et al., an "acid test" stimulus—a sound that is unambiguously synthetic (e.g., a simple sine wave or a sound from a classic video game)—will be included in the test set.10 This serves as an attention check to help screen out participants who are not engaging with the task seriously.Procedure: Participants will be seated in a quiet environment, preferably using high-quality headphones. In each trial, they will be presented with a single audio clip and must respond by classifying it as either "Recorded" or "Synthesized." The order of presentation of all stimuli will be randomized for each participant to control for order effects. Participants will be instructed to listen to each sound only once to encourage judgments based on initial perception.Participants: A panel of at least 20 listeners will be recruited. Participants will be asked to self-report their level of expertise in audio engineering and sound design on a 5-point Likert scale. This data will allow for an analysis of whether expertise correlates with the ability to discriminate between recorded and synthesized sounds, a factor that was found to be non-significant in a similar study.10Analysis: The primary data collected will be the classification accuracy for each stimulus. From this, two key metrics will be calculated: the discrimination factor (d') and the F-measure.10 These metrics provide a quantitative measure of how well participants can distinguish the synthetic tones from their recorded counterparts. A d' value below a pre-determined threshold (e.g., 0.75) is typically interpreted as the sounds being perceptually indistinguishable, with a value around 0.5 indicating performance no better than random chance.10 The null hypothesis will be rejected if the mean discrimination factor is significantly above this threshold.4.1.2 Alternative Methodology: MUSHRAAs an alternative or follow-up study, the MUSHRA (MUlti Stimulus with Hidden Reference and Anchor) methodology could be employed.34 MUSHRA is an ITU-R standard for assessing intermediate audio quality and is particularly well-suited for comparing multiple systems.36 In a MUSHRA test, listeners are presented with an open reference (the original recording), a hidden version of the reference, one or more low-quality "anchors," and the various systems under test (e.g., Eigensound, a sampler, Chromaphone).37 Listeners rate each system on a continuous 0-100 scale. This would allow for a more nuanced evaluation of Eigensound's quality relative to other synthesis technologies, rather than just in absolute terms against a recording.4.2 Study 2: Mixed-Methods Evaluation of Usability and InteractionWhile sonic quality is essential, a DMI's success is equally dependent on its playability and the quality of the user experience it provides. This is particularly true for an instrument like Eigensound, whose novelty is tied to its gestural interface. A powerful sound engine with a confusing or frustrating control scheme will ultimately fail as an instrument. This study is designed to provide a rich, two-pronged assessment of the novice user experience by combining the deep, contextual insights of qualitative inquiry with the standardized, benchmarkable data of a quantitative survey. This mixed-methods approach allows the "what" of the quantitative score to be explained by the "why" of the qualitative findings, yielding a more complete and actionable evaluation.4.2.1 Proposed MethodologyParticipants: A group of 15-20 participants will be recruited. To assess initial learnability and intuitiveness, these participants should be novices with respect to the Eigensound instrument. This group could include musicians who play other instruments as well as non-musicians, to gauge its accessibility to a wider audience.Procedure: The study will be conducted in individual sessions, divided into two phases, followed by the administration of a questionnaire. The entire session will be video-recorded for later analysis.Qualitative Data Collection: This phase is adapted directly from the methodology proposed by Wanderley et al. for evaluating DMIs from the performer's view.28Phase 1: Free Exploration (10 minutes): The participant is presented with the Eigensound instrument and invited to explore it freely, without any specific goals or instructions. This phase is crucial for observing emergent behaviors and understanding the user's initial mental model of the instrument.Phase 2: Guided Exploration (10 minutes): The researcher provides a brief, standardized set of instructions explaining the basic mapping between gestures (tap, position, pressure, drag) and sonic parameters. The participant is then given time to explore the instrument with this new knowledge.Post-Session Interview: Immediately following the exploration phases, a semi-structured interview is conducted. The interview will probe the user's experience, focusing on aspects like ease of use, discoverability of features, points of confusion or frustration, moments of enjoyment, and their understanding of the relationship between their actions and the resulting sound.Quantitative Data Collection: At the conclusion of the session, each participant will be asked to complete the System Usability Scale (SUS) questionnaire.32 The SUS is a robust, widely-used, and validated 10-item Likert scale survey that provides a reliable global measure of perceived usability.39Analysis:Qualitative Analysis: The audio from the interviews will be transcribed verbatim. These transcripts will then be subjected to Discourse Analysis (DA), following the five-step process of transcription, free association, itemization, reconstruction of the user's world, and examining context.28 This systematic analysis will identify recurring themes and patterns in the participants' experiences, such as "the system felt responsive," "I didn't understand what pressure was doing," or "it was tiring to use for a long time," echoing the types of findings in prior DMI studies.28Quantitative Analysis: The responses from the SUS questionnaire will be scored for each participant according to the standard calculation method, yielding a score between 0 and 100.38 The mean SUS score for the entire participant group will then be calculated. This score can be directly compared to the widely accepted industry average SUS score of 68.32 A score significantly above 68 would suggest above-average perceived usability, while a score below would indicate potential usability issues that need to be addressed. The qualitative themes will then be used to interpret and explain this quantitative result.4.3 Study 3: Quantitative Evaluation of Expressive PotentialThe final study addresses the highest-level goal of a DMI: its potential for musical expression. The term "expressivity," however, is notoriously vague and difficult to evaluate. To overcome this, this study operationalizes expressivity by breaking it down into a set of concrete, fundamental musical performance capabilities. This approach is grounded in research from music psychology and performance science, which has identified the key acoustic cues that performers manipulate to convey expressive intent.31 By designing a series of tasks focused on these specific cues, the study can quantitatively measure the instrument's affordances for expert-level musical control. This transforms the evaluation of expressivity from a subjective judgment into an empirical, defensible assessment.4.3.1 Proposed MethodologyParticipants: A panel of 8-12 expert musicians will be recruited. The selection criteria will depend on the specific interaction paradigm of Eigensound. For a multi-touch interface, this could include expert percussionists (especially hand-drummers), pianists, or musicians with significant experience using other touchscreen or gestural controllers. Their expertise ensures they possess the fine motor control necessary to explore the instrument's full potential.Procedure: The study will consist of a task-based performance evaluation, modeled on the work of Arfib, Wanderley, and colleagues.31 After a brief period of familiarization with the instrument, participants will be asked to perform a series of short, specific musical tasks. Each task will be designed to isolate and test control over a single dimension of musical expression.Example Tasks: The tasks will be structured around the five core cue-groups for musical expression:Dynamics: "Perform a smooth crescendo from the quietest possible sound (pianissimo) to the loudest possible sound (fortissimo) over a period of 5 seconds." and "Perform a series of 5 notes with a sforzando (sudden, strong) attack."Articulation: "Play the provided C-major scale using a short, detached (staccato) articulation." and "Play the same scale using a smooth, connected (legato) articulation."Timbre: "On a single sustained note, smoothly change the timbre from its 'dullest' or 'darkest' quality to its 'brightest' quality and back again."Pitch Control: "Perform a slow, controlled vibrato on a sustained G4 note." and "Perform a smooth glissando (slide) from C4 to C5."Timing: "Accurately perform the following rhythmic pattern at a tempo of 120 bpm."Data Collection: After attempting each task, the performer will immediately rate the instrument's performance on that specific task using a 5-point Likert scale (1: Not at all, 5: Very well). The questions will be direct and task-related, such as: "How well did the instrument allow you to control dynamics for this task?" or "How adequate was the relationship between your gesture and the resulting articulation?".31Analysis: The Likert scale ratings for each task will be collected from all participants. The data will be analyzed by calculating the mean and median scores for each expressive dimension (dynamics, articulation, timbre, etc.). This will produce a quantitative profile of Eigensound's expressive capabilities, clearly highlighting its strengths and weaknesses from an expert performer's perspective. For example, the instrument might score highly on timbral control but lower on fine-grained dynamic control, providing specific, actionable feedback for future development.5.0 Discussion and Future WorkThe formal description of the Eigensound instrument and the comprehensive three-part evaluation protocol proposed in this paper lay the groundwork for a significant research trajectory. The potential outcomes of these studies have direct implications for both the academic understanding of DMI design and the practical development of Eigensound into a mature musical product.Should Study 1 demonstrate that Eigensound's output is perceptually indistinguishable from acoustic recordings for certain sound classes, it would validate the computational efficiency of its novel resonator coupling model as a viable pathway to high-fidelity synthesis. If Study 3 reveals high scores for expressive control, particularly in dimensions like timbre and articulation that are well-suited to its gestural interface, it would position Eigensound as a uniquely expressive instrument within the landscape of PM synthesizers. The synthesis of these findings would represent a significant achievement: an instrument that closes the gap between computational cost and expressive potential.Crucially, the results of this research program are intended to directly inform the development of the final "Eigensound VST" product. The qualitative feedback from the novice user study (Study 2) will be invaluable for refining the user interface, improving the clarity of the mapping, and addressing any ergonomic concerns. For instance, if multiple users report confusion about the effect of sustained pressure, the visual feedback on the interface could be redesigned to make this parameter's state more explicit. If expert users in Study 3 struggle with precise dynamic control, the mapping curve for velocity and pressure could be adjusted to provide a more nuanced response. This iterative loop—from empirical evaluation to design refinement—is central to developing a successful and satisfying musical instrument.This paper acknowledges the current proof-of-concept (PoC) status of the Eigensound Lite instrument. The immediate future work is the execution of the three proposed studies, which will provide the empirical data needed to substantiate the claims made herein. Beyond this initial validation, several advanced research avenues present themselves. One promising direction would be to investigate long-term skill acquisition with Eigensound. By adapting the mapping disruption methodology proposed by O'Connor et al., where the instrument's sensitivity is subtly altered without the player's knowledge, one could quantitatively assess the extent to which performers develop a robust, unconscious competence with the interface over time.41 Such a study would provide deeper insights into the instrument's potential for virtuosity and its long-term viability as a performance tool.6.0 ConclusionThis paper has presented the Eigensound DMI, a novel instrument designed to navigate the enduring challenge of creating expressive, interactive, and computationally efficient tools for musical creation. We have provided two main contributions. The first is a formal architectural and technical description of the instrument, detailing its unique synthesis engine based on a computationally efficient model of coupled modal resonators, and its integration with a gestural, multi-touch interaction paradigm. The second, and equally important, contribution is the proposal of a rigorous, multi-faceted experimental protocol for the instrument's comprehensive evaluation. This three-part protocol, designed to assess perceptual quality, usability, and expressive potential, provides a clear and scientifically-grounded roadmap for validating the instrument's claims and guiding its evolution from a proof-of-concept into a mature musical and research tool. By combining innovations in synthesis with a commitment to robust evaluation, Eigensound offers a promising new direction in the ongoing pursuit of powerful and accessible digital musical instruments.7.0 References26 Miranda, E. R., & Wanderley, M. M. (Eds.). (2006). A Framework for the Evaluation of Digital Musical Instruments. In New digital musical instruments: control and interaction beyond the keyboard. A-R Editions, Inc.27 Sinyor, E. (2009). Design and Use of Two Digital Musical Instruments Based on Moving Mechanical Systems. Master's Thesis, McGill University.28 Nogueira, W., et al. (2010). Towards an evaluation methodology for digital music instruments considering performer's view: a case study. Proceedings of the International Conference on New Interfaces for Musical Expression.41 O'Connor, A., et al. (2020). Quantifying Skilled Action on an Unfamiliar Interface. Proceedings of the International Conference on New Interfaces for Musical Expression.42 Morreale, F., et al. (2022). A Survey of Digital and Electronic Musical Instrument Performers. Journal of New Music Research.29 Young, G. W., & Murphy, D. (2015). HCI Models for Digital Musical Instruments: Methodologies for Rigorous Testing of Digital Musical Instruments. Proceedings of the International Conference on Computer Music and Multidisciplinary Research (CMMR).7 Karplus, K., & Strong, A. (1983). Digital synthesis of plucked string and drum timbres. Computer Music Journal, 7(2), 43–55.17 Finch, D. (2024). The Wonder and Future Uses of Physical Modelling Synthesis. Daniel's Finch Blog.2 Audio Modeling. (2024). SWAM and Ambiente.8 Baby Audio. (n.d.). What is Physical Modeling?. Baby Audio Blog.5 Miraglia, D. (2024). Physical Modelling Synthesis 101 + Super Creative Tips. Unison Audio Blog.43 Rocchesso, D., et al. (2011). Perceptual Evaluation of Rolling Sound Synthesis. Acta Acustica united with Acustica, 97(5), 828-837.44 Simonetta, F., et al. (2022). A Perceptual Measure for Evaluating the Resynthesis of Automatic Music Transcriptions. Multimedia Tools and Applications.45 Lazzaro, J., & Wawrzynek, J. (2006). A framework for feature-based synthesis. Proceedings of the International Computer Music Conference.46 Koelsch, S. (2011). Toward a neural basis of music perception–a review and updated model. Frontiers in Psychology, 2, 110.10 Farnell, A., et al. (2019). Perceptual Evaluation of Modal Synthesis for Impact-Based Sounds. Proceedings of the Sound and Music Computing Conference (SMC).47 NIME Conference. (n.d.). NIME Proceedings Archive.48 NIME 2023. (2023). Conference Papers.49 Bela Blog. (2020). NIME 2020 & Bela Sponsorship.50 DAFx 2015. (2015). Proceedings of the 18th International Conference on Digital Audio Effects.51 DAFx 2020. (2021). Proceedings of the 23rd International Conference on Digital Audio Effects.52 DAFx 2023. (2023). Proceedings of the 26th International Conference on Digital Audio Effects.53 Juslin, P. N., & Laukka, P. (2003). Communication of emotions in vocal expression and music performance: Different channels, same code?. Psychological Bulletin, 129(5), 770–814.4 Wikipedia. (n.d.). Physical modelling synthesis.3 Dannenberg, R. B. (2013). Introduction to Computer Music: Physical Models. Carnegie Mellon University.9 Cook, P. R. (n.d.). Physically-Inspired Sonic Modeling (PhISM). Princeton Sound Lab.6 Bilbao, S., et al. (2019). Physical Modeling, Algorithms, and Sound Synthesis: The NESS Project. Computer Music Journal, 43(2-3), 15-33.12 Bilbao, S., et al. (2019). Large-Scale Physical Modeling Synthesis, Parallel Computing, and Musical Experimentation: The NESS Project in Practice. Computer Music Journal, 43(2-3), 34-50.54 Computer Music Journal. (n.d.). Journal Homepage.18 Tape Op. (2012). Morphine Additive Synthesizer/Chromaphone Modeling Synthesizer Review.21 MusicRadar. (2021). AAS Chromaphone 3 Review.20 AudioNewsRoom. (2021). Applied Acoustic Systems Chromaphone 3 Review.19 Plugin Boutique. (2021). AAS Chromaphone 3 - Overview. YouTube.14 MusicRadar. (2022). Modartt Pianoteq 8 Review.22 Bigwood, R. (2023). Modartt Pianoteq 8 Review. Sound on Sound.23 Opus Science Collective. (2021). The Piano Compromise Part 2: Pianoteq.13 Vanacoro, M. (2025). Audio Modeling SWAM Instruments: The Synth and Software Review. Synth and Software.15 Pete C. (2021). Review: SWAM All in Bundle by Audio Modeling. Sample Library Review.16 Bigwood, R. (2018). Audio Modeling SWAM Solo Instruments. Sound on Sound.25 Gearspace Forum. (2023). SWAM Instruments currently the only product line that has full control over pitch slide/portamento timing?.4 Wikipedia. (n.d.). Physical modelling synthesis.11 Kirn, P. (2024). Baby Audio's Atoms mass-spring physical modeling synth, tested. CDM.1 Wright, J., et al. (2017). UX in the C: A Meta-Review of UX Methods in Music Interaction Research. Proceedings of the International Conference on New Interfaces for Musical Expression.30 University of Oslo. (n.d.). Music, Mind, and Motion: New Interfaces for Musical Expression. FutureLearn.31 Arfib, D., et al. (2005). On interface expressivity: A player-based study. Proceedings of the International Computer Music Conference.33 Arfib, D., et al. (2002). A method to evaluate the expressivity of musical interfaces. Proceedings of the International Computer Music Conference.32 Zhou, L., et al. (2022). The System Usability Scale (SUS) for digital health apps: meta-analysis. JMIR mHealth and uHealth, 10(1), e31131.38 UXtweak Blog. (n.d.). System Usability Scale (SUS): A Comprehensive Guide.39 Usability Geek. (n.d.). How to Use the System Usability Scale (SUS) to Evaluate the Usability of Your Website.40 MeasuringU. (n.d.). Measuring Usability with the System Usability Scale (SUS).34 Wikipedia. (n.d.). MUSHRA.37 AudioLabs Erlangen. (n.d.). MUSHRA Listening Test Methodology.35 ITU-R Recommendation BS.1534-3. (2015). Method for the subjective assessment of intermediate quality level of audio systems.36 ITU-R Recommendation BS.1534-1. (2003). Method for the subjective assessment of intermediate quality level of audio systems.21 MusicRadar. (2021). AAS Chromaphone 3 Review.13 Vanacoro, M. (2025). Audio Modeling SWAM Instruments: The Synth and Software Review. Synth and Software.24 Audio Modeling. (n.d.). SWAM Engine.